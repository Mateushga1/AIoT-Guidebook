{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Native Tensorflow GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning TensorFlow with Hope,2017\n",
    "\n",
    "Following notebook is based in the bibliography:\n",
    "\n",
    "T. Hope, Y. S. Resheff, and I. Lieder, Learning TensorFlow: A Guide to Building Deep Learning Systems. Sebastopol, CA: O'Reilly Media, 2017.\n",
    "\n",
    "**First Example**\n",
    "\n",
    "Have installed an evironment for \n",
    "|OS      | Python | TensorFlow  | CUDA | cuDNN |\n",
    "|--------|--------|-------------|------|-------|\n",
    "| Linux  | 3.10   | 2.15.0+     | 12.1 | 8.9   |\n",
    "\n",
    "\n",
    "In order to print a \"Hello World!\", we could use the classic Python code for that. When TensorFLow is evoqued the text is now stored as a Tensor in tf object/class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Python location: \", sys.executable)\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(\"Is CUDA avaliable? \",tf.test.is_built_with_cuda())\n",
    "GPU=tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(GPU))\n",
    "if GPU:\n",
    "    try:\n",
    "        for gpu in GPU:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth set for GPU.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "h = tf.constant(\"Hello\")\n",
    "w = tf.constant(\" World!\")\n",
    "hw = h+w\n",
    "print(hw.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second Example**\n",
    "\n",
    "Softmax Regression is a simple classifiers using tf environment. The nomenclature used is often $w_i^j$, where $w$ is a vector; $i$ is the indice of the vector; $j$ is the class that $w$ represents. Finally, $W$ is the database of $[w^a...w^z] \\forall j \\in [a,...,z]$ avaliable classes. This softmax regression is used to train MNIST benchmark, whgihc has 9 classes as follows. To train a neural network using MNIST data base and using softmax, one may transform the image $[28x28]$ into a flaten vector of $[1 x 784]$. Thus the position of the pixels (black or white) does not matter anymore in the training. Softmax neural network has 9 outputs, one for each class, and they store the probability to the input being from such class. The most probable class must be taken. See figure below:\n",
    "\n",
    "<img src=\"../images/MNIST_dataset.png\" alt=\"MNIST Dataset\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.utils import plot_model\n",
    "from callbackPlots import AccuracyPlotCallback\n",
    "from callbackPlots import count_neurons_and_synapses\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the data (optional, but recommended)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "print(f\"Training set size: {x_train.shape}, Test set size: {x_test.shape}\")\n",
    "\n",
    "# Flatten images (28x28 -> 784)\n",
    "# Using this option the position of the pixels in the 2D space is not used\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Build a simple softmax classifier\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='softmax', input_shape=(784,))\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Create the callback instance\n",
    "plot_callback = AccuracyPlotCallback()\n",
    "\n",
    "# Debugging: Print if the callback is created correctly\n",
    "print(f\"Callback created: {plot_callback}\")\n",
    "\n",
    "# Train the model with the callback\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test), callbacks=[plot_callback])\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Save the model architecture as a PNG file\n",
    "plot_model(model, to_file=\"model_architecture.png\", show_shapes=True, show_layer_names=True, dpi=300)\n",
    "\n",
    "# Call the function\n",
    "count_neurons_and_synapses(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning and the Train/Test Scheme\n",
    "\n",
    "Supervised learning generally refers to the task of learning a function from data objects to labels associated with them, based on a set of examples where the correct labels are already known. This is usually subdivided into the case where labels are continuous (regression) or discrete (classification). \n",
    "\n",
    "The purpose of training supervised learning models is almost always to apply them later to new examples with unknown labels called inference phase, in order to obtain predicted labels for them. In the MNIST case discussed in this section, the purpose of training the model would probably be to apply it on new handwritten digit images and automatically find out what digits they represent.\n",
    "\n",
    "As a result, we are interested in the extent to which our model will label new examples correctly. This is reflected in the way we evaluate the accuracy of the model. We first partition the labeled dataset into train and test partitions. During model training, we use only the train partition, and during validation we test the accuracy only on the test partition. This scheme is generally known as a train/test validation.\n",
    "\n",
    "The actual training of the model, in the stochastic gradient descent (SGD) approach, consists of taking many steps in ‚Äúthe right direction.‚Äù The SGD optimizer used in this example is Adam. The cost function used is a cross entropy pre-defined fuction. The metric is the model accuracy, which is plotted during training and validation. \n",
    "\n",
    "In this example,we learn what is a epoch and a batch_size. To correclty define them, we have found.\n",
    "An epoch is one full pass over the entire training dataset. The number of epochs affects:\n",
    "\n",
    "‚úÖ Training Time:\n",
    "\n",
    "- More epochs = longer training time.\n",
    "- Too many epochs can lead to overfitting, where the model memorizes the training data instead of generalizing.\n",
    "\n",
    "‚úÖ Model Performance:\n",
    "\n",
    "- 5 epochs is often enough for simple datasets like MNIST because the softmax classifier is relatively simple.\n",
    "- More complex models may require more epochs to learn complex patterns.\n",
    "- If accuracy is low after 5 epochs, increasing the number of epochs (e.g., 10 or 20) can improve performance.\n",
    "\n",
    "üöÄ How to choose the right number of epochs?\n",
    "Use early stopping to stop training when the validation accuracy stops improving. Example:\n",
    "```python\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test), callbacks=[early_stopping])\n",
    "```\n",
    "\n",
    "The batch size determines how many training samples are processed before updating the model's weights.\n",
    "\n",
    "‚úÖ Effect on Memory:\n",
    "\n",
    "- A larger batch size (e.g., 128, 256) requires more GPU/CPU memory but speeds up training.\n",
    "- A smaller batch size (e.g., 16 or 32) takes longer but uses less memory.\n",
    "\n",
    "‚úÖ Effect on Model Performance:\n",
    "\n",
    "- Smaller batch sizes (e.g., 32, 64) ‚Üí More stable updates, better generalization.\n",
    "- Larger batch sizes (e.g., 128, 256) ‚Üí Faster training but may generalize worse.\n",
    "\n",
    "üöÄ How to choose the right batch size?\n",
    "\n",
    "-  32 or 64 is a good starting point for most problems. <- Golden Number from Rule of Thumb\n",
    "- If training is slow, increase batch size (but check GPU memory usage).\n",
    "- If results fluctuate a lot, try smaller batch sizes (e.g., 16).\n",
    "\n",
    "The result is a neural network model described as:\n",
    "- Input Layer: 784 neurons (flattened 28√ó28 images)\n",
    "- Dense Output Layer: 10 neurons (softmax activation for classification)\n",
    "- This is a fully connected layer, every input neuron connects to every neuron in the next layer:\n",
    "\n",
    "$Synapses=Input Neurons√óOutput Neurons+Bias Terms$\n",
    "\n",
    "$Synapses=(784√ó10)+10=7850$\n",
    "\n",
    "[ Input Layer ] ‚Üí  [ Dense Layer (784 neurons) ] ‚Üí [ Output Layer (10 neurons, softmax) ]\n",
    "\n",
    "<img src=\"../images/model_architecture.png\" alt=\"NN model\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Tensorflow and Graph Computation\n",
    "\n",
    "Roughly speaking, working with TensorFlow involves two main phases: (1) constructing a graph and (2) executing it. Let‚Äôs jump into our first example and create\n",
    "something very basic.\n",
    "\n",
    "Graph Operations are described in the following table.\n",
    "\n",
    "| TensorFlow Operator    | Shortcut    | Description                                                             |\n",
    "|------------------------|-------------|-------------------------------------------------------------------------|\n",
    "| tf.add()               | a + b       | Adds a and b, element-wise.                                             |\n",
    "| tf.multiply()          | a * b       | Multiplies a and b, element-wise.                                       |\n",
    "| tf.subtract()          | a - b       | Subtracts a from b, element-wise.                                       |\n",
    "| tf.divide()            | a / b       | Computes Python-style division of a by b.                               |\n",
    "| tf.pow()               | a ** b      | Returns the result of raising each element in a to its corresponding element b, element-wise. |\n",
    "| tf.mod()               | a % b       | Returns the element-wise modulo.                                        |\n",
    "| tf.logical_and()       | a & b       | Returns the truth table of a & b, element-wise. dtype must be tf.bool.  |\n",
    "| tf.greater()           | a > b       | Returns the truth table of a > b, element-wise.                         |\n",
    "| tf.greater_equal()     | a >= b      | Returns the truth table of a >= b, element-wise.                        |\n",
    "| tf.less_equal()        | a <= b      | Returns the truth table of a <= b, element-wise.                        |\n",
    "| tf.less()              | a < b       | Returns the truth table of a < b, element-wise.                         |\n",
    "| tf.negative()          | -a          | Returns the negative value of each element in a.                        |\n",
    "| tf.logical_not()       | ~a          | Returns the logical NOT of each element in a. Only compatible with Tensor objects with dtype of tf.bool. |\n",
    "| tf.abs()               | abs(a)      | Returns the absolute value of each element in a.                        |\n",
    "| tf.logical_or()        | a \\| b       | Returns the truth table of a | b, element-wise. dtype must be tf.bool.  |\n",
    "\n",
    "Let's do the graph example illustrated in the figure below\n",
    "\n",
    "<img src=\"graph_example.png\" alt=\"NN model\" width=\"500\">\n",
    "\n",
    "The Python code should be this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(5)\n",
    "b = tf.constant(2)\n",
    "c = tf.constant(3)\n",
    "\n",
    "d = tf.multiply(a,b)\n",
    "e = tf.add(c,b)\n",
    "f = tf.subtract(d,e)\n",
    "\n",
    "outs = f.numpy()  # To get the value of the tensor in eager execution mode\n",
    "print(\"outs = {}\".format(outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Basic Functions\n",
    "\n",
    "In our initial graph example, we request one specific node (node f) by passing the\n",
    "variable it was assigned to as an argument to the sess.run() method. This argument\n",
    "is called fetches, corresponding to the elements of the graph we wish to compute.\n",
    "\n",
    "In TensorFlow 2.x, the concept of graphs is abstracted away as TensorFlow 2.x operates in eager execution by default, and you no longer need to create and manage graphs manually like you did in TensorFlow 1.x. However it is interesting to understand how it works using the graph abstraction. The default behavior is eager execution,  and all operations happen immediately (eagerly), and you don‚Äôt need to manage a session or a graph manually. If you still want to use a graph in TensorFlow 2.x, you can enable graph execution by using tf.function, which creates a graph behind the scenes for optimization purposes. Be aware of your code readbility!\n",
    "\n",
    "Here some comon functions from tensorflow library\n",
    "| TensorFlow Operation                        | Description |\n",
    "|---------------------------------------------|-------------|\n",
    "| `tf.constant(value)`                        | Creates a tensor populated with the specified value(s). |\n",
    "| `tf.fill(shape, value)`                     | Creates a tensor of shape `shape` and fills it with `value`. |\n",
    "| `tf.zeros(shape)`                           | Returns a tensor of shape `shape` with all elements set to `0`. |\n",
    "| `tf.zeros_like(tensor)`                     | Returns a tensor of the same type and shape as `tensor` with all elements set to `0`. |\n",
    "| `tf.ones(shape)`                            | Returns a tensor of shape `shape` with all elements set to `1`. |\n",
    "| `tf.ones_like(tensor)`                      | Returns a tensor of the same type and shape as `tensor` with all elements set to `1`. |\n",
    "| `tf.random_normal(shape, mean, stddev)`     | Outputs random values from a normal distribution. |\n",
    "| `tf.truncated_normal(shape, mean, stddev)`  | Outputs random values from a truncated normal distribution (values whose magnitude is more than two standard deviations from the mean are dropped and re-picked). |\n",
    "| `tf.random_uniform(shape, minval, maxval)`  | Generates values from a uniform distribution in the range `[minval, maxval)`. |\n",
    "| `tf.random_shuffle(tensor)`                 | Randomly shuffles a tensor along its first dimension. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#Matrix operation example\n",
    "A = tf.constant([ [1,2,3],[4,5,6] ])\n",
    "print(A.get_shape())\n",
    "x = tf.constant([1,0,1])\n",
    "print(x.get_shape())\n",
    "x = tf.expand_dims(x,1)\n",
    "print(x.get_shape())\n",
    "b = tf.matmul(A,x)\n",
    "print(\"Ax = {}\".format(b.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using `tf.Variable`**\n",
    "\n",
    "A TensorFlow variable is a tensor that can hold and update state (i.e., its value can change over time). Unlike ```python tf.constant()```, which creates an immutable tensor, ```python tf.Variable``` allows modification. Gradient Tracking for Training: tf.Variable is commonly used in training models because TensorFlow automatically tracks it for gradients, as in \n",
    "```python \n",
    "w = tf.Variable(tf.random.normal([3, 3]), trainable=True)\n",
    "```\n",
    "Here, ```python trainable=True``` ensures that TensorFlow optimizes the variable during backpropagation.\n",
    "\n",
    "**Third Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a variable\n",
    "x = tf.Variable(10, dtype=tf.int32)\n",
    "\n",
    "# Print the initial value\n",
    "print(\"Initial value:\", x.numpy())\n",
    "\n",
    "# Update the value\n",
    "x.assign(20)\n",
    "print(\"Updated value:\", x.numpy())\n",
    "\n",
    "# Increment by 5\n",
    "x.assign_add(5)\n",
    "print(\"After increment:\", x.numpy())\n",
    "\n",
    "# Decrement by 3\n",
    "x.assign_sub(3)\n",
    "print(\"After decrement:\", x.numpy())\n",
    "\n",
    "#TensorFlow automatically tracks it for gradients, like in synapses weights\n",
    "w = tf.Variable(tf.random.normal([3, 3]), trainable=True)\n",
    "print(\"weights value:\", w.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Tensorflow Models\n",
    "\n",
    "TensorFlow, however,has designated built-in structures for feeding input values. These structures are called placeholders. Placeholders can be thought of as empty Variables that will be filled with data later on. We use them by first constructing our graph and only when it is executed feeding them with the input data. In TensorFlow 1.x, `tf.placeholder()` was used to create input tensors that would be fed values at runtime using feed_dict inside a `tf.Session()`. Since eager execution allows immediate computation, you can simply use function arguments or `tf.function` for graphs. Therefore, placeholder does not exist anymore.\n",
    "\n",
    "\n",
    "1Ô∏è‚É£ What is `tf.function()`?\n",
    "\n",
    "`tf.function()` is a decorator in TensorFlow 2.x that converts a Python function into a TensorFlow computation graph (also called a \"Graph Execution\" mode). This makes TensorFlow faster and more optimized compared to the default Eager Execution.\n",
    "\n",
    "üîπ Eager Execution (Default in TF 2.x): Operations are executed immediately.\n",
    "\n",
    "üîπ Graph Execution (`tf.function`): TensorFlow compiles the function into a graph, optimizing it for speed.\n",
    "\n",
    "2Ô∏è‚É£ Why Use tf.function()?\n",
    "\n",
    "‚úÖ Faster Execution: Converts the function into a TensorFlow graph for optimized execution.\n",
    "\n",
    "‚úÖ Automatic Graph Optimization: TensorFlow optimizes the graph internally for performance.\n",
    "\n",
    "‚úÖ Runs on GPU/TPU Efficiently: Graph execution is much faster on GPUs & TPUs.\n",
    "\n",
    "‚úÖ Reduces Python Overhead: Removes unnecessary Python operations for efficiency.\n",
    "\n",
    "When you use `@tf.function`, TensorFlow compiles your Python function into a computation graph (also called Graph Execution). This graph is optimized for faster execution compared to normal Python functions (Eager Execution).\n",
    "üìå What Happens Internally?\n",
    "\n",
    "When you run `model(x_new)`, TensorFlow does the following steps behind the scenes:\n",
    "\n",
    "1Ô∏è‚É£ Function Tracing (First Call)\n",
    "\n",
    "- The first time `model(x_new)` is called, TensorFlow analyzes the Python function.\n",
    "- It converts TensorFlow operations (like `tf.matmul`) into a graph representation.\n",
    "- This process is called \"tracing\" and results in a TensorFlow computational graph.\n",
    "\n",
    "‚è≥ This step happens only once (on the first call). The function is then cached for reuse.\n",
    "\n",
    "2Ô∏è‚É£ Graph Compilation & Optimization\n",
    "\n",
    "- After tracing, TensorFlow compiles the graph.\n",
    "- It optimizes operations (e.g., removing redundant calculations, fusing operations together).\n",
    "- The resulting graph is much faster than normal Python execution.\n",
    "\n",
    "3Ô∏è‚É£ Graph Execution (On Subsequent Calls)\n",
    "\n",
    "- The next time you call `model(x_new)`, TensorFlow doesn‚Äôt execute the Python code again!\n",
    "- Instead, it reuses the compiled computation graph for maximum efficiency.\n",
    "- This is why `@tf.function` speeds up training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Linear Graph for a Neural Network\n",
    "\n",
    "We have some target variable y, which we want to explain using some feature vector\n",
    "x. To do so, we first choose a model that relates the two. Our training data points will\n",
    "be used for ‚Äútuning‚Äù the model so that it best captures the desired relation. In the fol‚Äê\n",
    "lowing chapters we focus on deep neural network models, but for now we will settle\n",
    "for a simple regression problem.\n",
    "Let‚Äôs start by describing our regression model:\n",
    "\n",
    "$f(x_i) = w^Tx_i +b$\n",
    "\n",
    "$y_i = f(x_i) +\\epsilon_i$\n",
    "\n",
    "$f(x_i)$ is assumed to be a linear combination of some input data xi, with a set of\n",
    "weights $w$ and an intercept $b$. Our target output $y_i$ is a noisy version of $f(x_i)$ after being\n",
    "summed with Gaussian noise $\\epsilon_i$ (where $i$ denotes a given sample).\n",
    "\n",
    "# What is a Loss Function ?\n",
    "It a good measure with which we can evaluate the model‚Äôs performance.To capture the discrepancy between our model‚Äôs predictions and the observed targets, we need a measure reflecting ‚Äúdistance.‚Äù This distance is often referred to as an objective or a loss function, and we optimize the model by finding the set of parameters (weights and bias in this case) that minimize it.\n",
    "\n",
    "Perhaps the most commonly used loss is the MSE (mean squared error), where for all\n",
    "samples we average the squared distances between the real target and what our model\n",
    "predicts across samples:\n",
    "\n",
    "$L(y, \\hat{y}) = \\frac {1}{n} \\sum^{n}_{i=1}\\left( y_i - \\hat{y}_i\\right)^2$\n",
    "\n",
    "Which turns in Python to:\n",
    "```python\n",
    "loss = tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "```\n",
    "\n",
    "Another very common loss, especially for categorical data, is the cross entropy, which\n",
    "we used in the softmax classifier in the previous chapter. The cross entropy is given\n",
    "by\n",
    "\n",
    "$H(p,q) = - \\sum_x p(x)\\log{q(x)}$\n",
    "\n",
    "and for classification with a single correct label (as is the case in an overwhelming\n",
    "majority of the cases) reduces to the negative log of the probability placed by the classifier on the correct label.\n",
    "\n",
    "Which turns in Python to:\n",
    "```python\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true,logits=y_pred)\n",
    "loss = tf.reduce_mean(loss)\n",
    "```\n",
    "\n",
    "Cross entropy is a measure of similarity between two distributions. Since the classifi‚Äê\n",
    "cation models used in deep learning typically output probabilities for each class, we\n",
    "can compare the true class (distribution $p$) with the probabilities of each class given\n",
    "by the model (distribution $q$). The more similar the two distributions, the smaller our\n",
    "cross entropy will be.\n",
    "\n",
    "# Gradient Descent Optimizer: how to train neural networks?\n",
    "\n",
    "While in some cases it is possible to find the global minimum analytically (when it exists), in\n",
    "the great majority of cases we will have to use an optimization algorithm. Optimizers\n",
    "update the set of weights iteratively in a way that decreases the loss over time.\n",
    "\n",
    "*Lemma:*\n",
    "\n",
    "So if $\\hat{w}_1 = \\hat{w}_0-\\gamma \\nabla F(\\hat{w}_0)$ where $\\nabla F(\\hat{w}_0)$ is the gradient of $F$ evaluated at $\\hat{w}_0$, then for a\n",
    "small enough $\\gamma$:\n",
    "\n",
    "$F(\\hat{w}_0) \\geq F(\\hat{w}_1)$\n",
    "\n",
    "While convergence to the global minimum is guaranteed for convex functions, for nonconvex problems\n",
    "(which are essentially all problems in the world of deep learning) they can get stuck\n",
    "in local minima. In practice, this is often good enough, as is evidenced by the huge\n",
    "success of the field of deep learning.\n",
    "\n",
    "A more popular technique is the *stochastic gradient descent (SGD)*, where instead of\n",
    "feeding the entire dataset to the algorithm for the computation of each step, a subset\n",
    "of the data is sampled sequentially. The number of samples ranges from one sample at\n",
    "a time to a few hundred, but the most common sizes are between around 50 to\n",
    "around 500 (usually referred to as mini-batches).\n",
    "\n",
    "Using smaller batches usually works faster, and the smaller the size of the batch, the\n",
    "faster are the calculations. However, there is a trade-off in that small samples lead to\n",
    "lower hardware utilization and tend to have high variance, causing large fluctuations\n",
    "to the objective function. Nevertheless, it turns out that some fluctuations are benefi‚Äê\n",
    "cial since they enable the set of parameters to jump to new and potentially better local\n",
    "minima. Using a relatively smaller batch size is therefore effective in that regard, and\n",
    "is currently overall the preferred approach.\n",
    "\n",
    "An important parameter to set is the algorithm‚Äôs learning rate, determining how\n",
    "aggressive each update iteration will be (or in other words, how large the step will be\n",
    "in the direction of the negative gradient). We want the decrease in the loss to be fast\n",
    "enough on the one hand, but on the other hand not large enough so that we over-\n",
    "shoot the target and end up at a point with a higher value of the loss function.\n",
    "\n",
    "**Fourth Example: linear regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "# Set up Matplotlib configuration (font family, etc.)\n",
    "matplotlib.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman'],\n",
    "    'text.usetex': True  # Use LaTeX for all text rendering\n",
    "})\n",
    "\n",
    "\n",
    "# Trainable variables (weights & bias)\n",
    "w = tf.Variable(tf.random.normal([1, 3]), trainable=True,name='weights')\n",
    "b = tf.Variable(0.1, dtype=tf.float32, trainable=True, name='bias')\n",
    "noise = np.random.randn(2, 1) * 0.1  # Adjust noise to match shape\n",
    "\n",
    "@tf.function\n",
    "def model(x):\n",
    "    return tf.matmul(x,tf.transpose(w)) + b + noise\n",
    "    \n",
    "# Example: Providing `x` dynamically\n",
    "x_new = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=tf.float32)  # Dynamic input\n",
    "y_true = tf.constant([[4.0], [10.0]]) \n",
    "y_pred = model(x_new)\n",
    "\n",
    "\n",
    "# **Switch to Mean Squared Error (MSE) Loss** for regression\n",
    "def compute_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))  # MSE Loss\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    abs_percentage_error = tf.abs((y_pred - y_true) / y_true)  # Absolute percentage error\n",
    "    mape = tf.reduce_mean(abs_percentage_error)  # Compute mean\n",
    "    accuracy = 1 - mape # accuracy based on MAPE\n",
    "    return tf.clip_by_value(accuracy, 0, 1)\n",
    "\n",
    "\n",
    "#Training phase\n",
    "learning_rate = 0.0001\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "# Training phase (multiple steps)\n",
    "epochs = 500  # Number of training steps\n",
    "\n",
    "# List to store the loss values for plotting\n",
    "loss_values = []\n",
    "accuracy_values = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Record operations for gradient computation\n",
    "        y_pred = model(x_new)  # Forward pass: model prediction\n",
    "        loss = compute_loss(y_true, y_pred)  # Compute the loss\n",
    "        accuracy = compute_accuracy(y_true, y_pred)  # Compute the loss\n",
    "        \n",
    "    # Compute gradients using the tape\n",
    "    gradients = tape.gradient(loss, [w, b])\n",
    "\n",
    "    # Apply gradients to update variables\n",
    "    optimizer.apply_gradients(zip(gradients, [w, b]))\n",
    "    if epoch % 50 == 0:  # Print every 50 epochs\n",
    "            loss_values.append(loss.numpy())\n",
    "            accuracy_values.append(accuracy.numpy())\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}, Accuracy: {accuracy.numpy()}\")\n",
    "\n",
    "# Plotting the loss values\n",
    "plt.plot(range(0, epochs, 50), loss_values, marker='o')\n",
    "plt.title(\"Loss vs Epochs\", fontsize=14)\n",
    "plt.xlabel(\"Epochs\", fontsize=24)\n",
    "plt.ylabel(\"Loss\", fontsize=24)\n",
    "plt.tick_params(axis='y', labelsize=20)  # Same font size for y-axis ticks\n",
    "plt.tick_params(axis='x', labelsize=20)  # Same font size for x-axis ticks\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plotting the loss values\n",
    "plt.plot(range(0, epochs, 50), accuracy_values, marker='*')\n",
    "plt.title(\"Accuracy vs Epochs\", fontsize=14)\n",
    "plt.xlabel(\"Epochs\", fontsize=24)\n",
    "plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "plt.tick_params(axis='y', labelsize=20)  # Same font size for y-axis ticks\n",
    "plt.tick_params(axis='x', labelsize=20)  # Same font size for x-axis ticks\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Final loss after training\n",
    "print(f\"Final Loss: {loss.numpy()}\")\n",
    "# Final accuracy after training\n",
    "print(f\"Final Accuracy: {accuracy.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fifth Example: logistic regression**\n",
    "\n",
    "Here, the linear component $f(x_i) = w^Tx_i +b$ is the input of a nonlinear function called the logistic\n",
    "function described as \n",
    "\n",
    "$P_r\\left(y_i=1|x_i\\right) = \\frac{1}{1+ e^{wx_i +b}}$\n",
    "\n",
    "We then regard these values as probabilities from which binary yes/1 or no/0 outcomes are generated. This is the nondeterministic (noisy) part of the model.\n",
    "The logistic function is more general, and can be used with a different set of parameters for the steepness of the curve and its maximum value. This special case of a logistic function we are using is also referred to as a sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "# Set up Matplotlib configuration (font family, etc.)\n",
    "matplotlib.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman'],\n",
    "    'text.usetex': True  # Use LaTeX for all text rendering\n",
    "})\n",
    "\n",
    "\n",
    "# Trainable variables (weights & bias)\n",
    "w = tf.Variable(tf.random.normal([1, 3]), trainable=True,name='weights')\n",
    "b = tf.Variable(0.1, dtype=tf.float32, trainable=True, name='bias')\n",
    "noise = np.random.randn(2, 1) * 0.1  # Adjust noise to match shape\n",
    "\n",
    "@tf.function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + tf.math.exp(-x))\n",
    "    \n",
    "@tf.function\n",
    "def model(x):\n",
    "    logits = tf.matmul(x, tf.transpose(w)) + b  + noise # Keep raw logits\n",
    "    return tf.sigmoid(logits)\n",
    "\n",
    "    \n",
    "# Example: Providing `x` dynamically\n",
    "x_new = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=tf.float32)  # Dynamic input\n",
    "y_true = tf.constant([[0.7], [0.9]]) #y_i is now a probability bounded in [0,1] and I have 2 outputs in my neural network\n",
    "y_pred = model(x_new)\n",
    "\n",
    "\n",
    "# ‚úÖ Fix loss function: Use Binary Cross-Entropy with probabilities\n",
    "def compute_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.binary_crossentropy(y_true, y_pred)  # Proper BCE loss\n",
    "    \n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    abs_percentage_error = tf.abs((y_pred - y_true) / y_true)  # Absolute percentage error\n",
    "    mape = tf.reduce_mean(abs_percentage_error)  # Compute mean\n",
    "    accuracy = 1 - mape # accuracy based on MAPE\n",
    "    return tf.clip_by_value(accuracy, 0, 1)\n",
    "\n",
    "\n",
    "#Training phase\n",
    "learning_rate = 0.002 # The learning rate should be bigger for strong nonlinearities\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "# Training phase (multiple steps)\n",
    "epochs = 400  # Number of training steps is lesser if the learning rate is bigger\n",
    "\n",
    "# List to store the loss values for plotting\n",
    "loss_values = []\n",
    "accuracy_values = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Record operations for gradient computation\n",
    "        y_pred = model(x_new)  # Forward pass: model prediction\n",
    "        loss = compute_loss(y_true, y_pred)  # Compute the loss\n",
    "        accuracy = compute_accuracy(y_true, y_pred)  # Compute the loss\n",
    "        \n",
    "    # Compute gradients using the tape\n",
    "    gradients = tape.gradient(loss, [w, b])\n",
    "\n",
    "    # Apply gradients to update variables\n",
    "    optimizer.apply_gradients(zip(gradients, [w, b]))\n",
    "    if epoch % 20 == 0:  # Print every 20 epochs\n",
    "            loss_values.append(loss.numpy())\n",
    "            accuracy_values.append(accuracy.numpy())\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}, Accuracy: {accuracy.numpy()}\")\n",
    "\n",
    "# Plotting the loss values\n",
    "plt.plot(range(0, epochs, 20), loss_values, marker='o')\n",
    "plt.title(\"Loss vs Epochs\", fontsize=14)\n",
    "plt.xlabel(\"Epochs\", fontsize=24)\n",
    "plt.ylabel(\"Loss\", fontsize=24)\n",
    "plt.tick_params(axis='y', labelsize=20)  # Same font size for y-axis ticks\n",
    "plt.tick_params(axis='x', labelsize=20)  # Same font size for x-axis ticks\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plotting the loss values\n",
    "plt.plot(range(0, epochs, 20), accuracy_values, marker='*')\n",
    "plt.title(\"Accuracy vs Epochs\", fontsize=14)\n",
    "plt.xlabel(\"Epochs\", fontsize=24)\n",
    "plt.ylabel(\"Accuracy\", fontsize=24)\n",
    "plt.tick_params(axis='y', labelsize=20)  # Same font size for y-axis ticks\n",
    "plt.tick_params(axis='x', labelsize=20)  # Same font size for x-axis ticks\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Final loss after training\n",
    "print(f\"Final Loss: {loss.numpy()}\")\n",
    "# Final accuracy after training\n",
    "print(f\"Final Accuracy: {accuracy.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîé How to Choose the Best Loss Function?\n",
    "\n",
    "The best loss function depends on **your problem type** and **the output of your model**. Here's a breakdown:\n",
    "\n",
    "## üìå 1. What Type of Problem Are You Solving?\n",
    "\n",
    "### (A) Classification (Predicting Categories or Probabilities)\n",
    "\n",
    "| Problem Type | Model Output | Best Loss Function |\n",
    "|-------------|-------------|--------------------|\n",
    "| **Binary Classification** (e.g., spam/not spam) | Probability (0 to 1) | `binary_crossentropy` |\n",
    "| **Multi-Class Classification** (e.g., dog/cat/rabbit) | Probabilities for each class | `categorical_crossentropy` |\n",
    "| **Multi-Class (One Label at a Time)** | Logits (Raw Scores) | `sparse_categorical_crossentropy` |\n",
    "\n",
    "üí° **Example:** If you're predicting a probability (like in your sigmoid model), use **Binary Cross-Entropy (`binary_crossentropy`)**.\n",
    "\n",
    "---\n",
    "\n",
    "### (B) Regression (Predicting Continuous Numbers)\n",
    "\n",
    "| Problem Type | Model Output | Best Loss Function |\n",
    "|-------------|-------------|--------------------|\n",
    "| **Predicting Any Real Number** | Continuous Value | `mean_squared_error (MSE)` |\n",
    "| **Predicting Close to Zero Differences** | Continuous Value | `mean_absolute_error (MAE)` |\n",
    "| **Handling Outliers Well** | Continuous Value | `huber_loss` or `log_cosh` |\n",
    "\n",
    "üí° **Example:** If you are predicting a number (not a probability), use **MSE for smooth errors** or **MAE for robust errors**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 2. Does Your Model Output Logits or Probabilities?\n",
    "\n",
    "- **If using `softmax()` or `sigmoid()`, use BCE or Categorical Cross-Entropy.**\n",
    "- **If using raw logits, use `sigmoid_cross_entropy_with_logits()` or `categorical_crossentropy(from_logits=True)`.**\n",
    "\n",
    "---\n",
    "\n",
    "## üî¥ üö® Common Mistake:\n",
    "If you use `sigmoid()` **inside** your model and then apply `sigmoid_cross_entropy_with_logits()`, you **double apply sigmoid** and mess up learning. Instead:\n",
    "\n",
    "‚úÖ **Use `binary_crossentropy()` for probabilities.**  \n",
    "‚úÖ **Use `sigmoid_cross_entropy_with_logits()` only for raw logits.**\n",
    "\n",
    "<style>\n",
    "  div {\n",
    "    font-size: 10pt;\n",
    "  }\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Convolutional Neural Networks ?\n",
    "\n",
    "The fundamental difference between fully connected and convolutional neural net‚Äê\n",
    "works is the pattern of connections between consecutive layers. In the fully connected\n",
    "case, as the name might suggest, each unit is connected to all of the units in the previous layer.\n",
    "\n",
    "In the first MNIST example all 10 outputs are connected to all 784 pixels input in a 1D manner. In a convolutional layer of a neural network, on the other hand, each unit is connec‚Äê\n",
    "ted to a (typically small) number of nearby units in the previous layer. Furthermore,\n",
    "all units are connected to the previous layer in the same way, with the exact same\n",
    "weights and structure. Graphically the differnce is illustrated in the figure below.\n",
    "\n",
    "<img src=\"../images/CNN_FullyConnected.png\" alt=\"CNN versus FullyConnected\" width=\"800\">\n",
    "\n",
    "There are motivations commonly cited as leading to the CNN approach, coming\n",
    "from different schools of thought. The first angle is the so-called neuroscientific\n",
    "inspiration behind the model. The second deals with insight into the nature of\n",
    "images, and the third relates to learning theory.\n",
    "\n",
    "The Nobel Prize‚Äìwinning neurophysiologists Hubel and Wiesel discovered as early as\n",
    "the 1960s that the first stages of visual processing in the brain consist of application of\n",
    "the same local filter (e.g., edge detectors) to all parts of the visual field. The current\n",
    "understanding in the neuroscientific community is that as visual processing proceeds,\n",
    "information is integrated from increasingly wider parts of the input, and this is done\n",
    "hierarchically.\n",
    "\n",
    "Convolutional neural networks follow the same pattern. Each convolutional layer\n",
    "looks at an increasingly larger part of the image as we go deeper into the network.\n",
    "Most commonly, this will be followed by fully connected layers that in the biologically\n",
    "inspired analogy act as the higher levels of visual processing dealing with global\n",
    "information.\n",
    "\n",
    "From my point of view, there is an argument on second angle and thus pattern recoginition is better treated by CNN. the neuroscientific inspiration is probably not true if we consider the literature of spiking neural networks (SNN). Understanding the argument from Hope the neuroscientific inspiration is not a brain like approach, but more deeply how eye detects patterns. Thus, both first and second angles compleate themselves. In this view, a\n",
    "convolutional neural network layer computes the same features of an image, across\n",
    "all spatial areas. It can be seen as a regularization mechanism. Convolutional layers are often combined to fully connected ones to describe an architecture of neural network.\n",
    "\n",
    "The convolution operation is the fundamental means by which layers are connected in convolutional neural networks. It is performed in Tensorflow by\n",
    "```python\n",
    "tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "```\n",
    "where x is the data‚Äîthe input image, , or a downstream feature map obtained further\n",
    "along in the network, after applying previous convolution layers.\n",
    "\n",
    "# Activation Functions\n",
    "Following linear layers, whether convolutional or fully connected,\n",
    "it is common practice to apply nonlinear activation functions. One practical aspect of activation\n",
    "functions is that consecutive linear operations can be replaced by a\n",
    "single one, and thus depth doesn‚Äôt contribute to the expressiveness\n",
    "of the model unless we use nonlinear activations between the linear\n",
    "layers. In other words, linear layers could be represented by a single layer like in the second example.\n",
    "Typical nonlinear activation functions are illustrated in the figure below.\n",
    "\n",
    "<img src=\"../images/nonlinear_activation_functions.png\" alt=\"Nonlinear Activation Functions\" width=\"800\">\n",
    "\n",
    "# Pooling\n",
    "It is common to follow convolutional layers with pooling of outputs. Technically,\n",
    "pooling means reducing the size of the data with some local aggregation function, typ‚Äê\n",
    "ically within each feature map. The pooling reduces the size of the data to be processed downstream. This\n",
    "can drastically reduce the number of overall parameters in the model, especially if we\n",
    "use fully connected layers after the convolutional ones. Second reason is to compute\n",
    "features (patterns) not to care about small changes in position in an image. It is performed in Tensorflow by\n",
    "```python\n",
    "tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "```\n",
    "where the predefined size is (2x2) controlled by ```ksize``` argument, while ```strides``` controls by how much we ‚Äúslide‚Äù the pooling grids.\n",
    "\n",
    "# Dropout\n",
    "Dropout is a regularization trick\n",
    "used in order to force the network to distribute the learned representation across all\n",
    "the neurons. This process is often thought of as training an\n",
    "‚Äúensemble‚Äù of multiple networks, thereby increasing generalization. It is performed in Tensorflow by\n",
    "```python\n",
    "tf.nn.dropout(layer, keep_prob=keep_prob)\n",
    "```\n",
    "In order to be able to change ```keep_prob``` and control the dropout, we will use a tf.Variable and pass\n",
    "one value for train (.5) and another for test (1.0).\n",
    "\n",
    "**Sixth Example MNIST Take II**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from callbackPlots import AccuracyPlotCallback\n",
    "from callbackPlots import count_neurons_and_synapses\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (5, 5), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    #This specifies the convolution we will typically use. A full convolution (no skips) with an output the same size as the input.\n",
    "    #This function is used in the convolutional layer above\n",
    "    layers.Conv2D(64, (5, 5), activation='relu', padding='same'),\n",
    "    \n",
    "    #This sets the max pool to half the size across the height/width dimensions, and in total a quarter the size of the feature map.\n",
    "    #Execute the Pooling between convolutional layers to reduce the size of the data\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    #This is the actual layer we will use. Linear convolution as defined in conv2d, with a bias, followed by the ReLU nonlinearity.\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    \n",
    "    layers.Dropout(0.3),  # Dropout to prevent overfitting\n",
    "    layers.Dense(10, activation='softmax')  # Softmax for classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])  # Explicitly add metrics\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize images to [0,1] range\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Reshape to add channel dimension (28,28,1)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"Training set shape: {x_train.shape}, Test set shape: {x_test.shape}\")\n",
    "\n",
    "# Create the callback instance\n",
    "plot_callback = AccuracyPlotCallback()\n",
    "\n",
    "# Debugging: Print if the callback is created correctly\n",
    "print(f\"Callback created: {plot_callback}\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test), callbacks=[plot_callback])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Call the function\n",
    "count_neurons_and_synapses(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10\n",
    "CIFAR10 is another dataset with a long history in computer vision and machine\n",
    "learning. Like MNIST, it is a common benchmark that various methods are tested\n",
    "against. CIFAR10 is a set of 60,000 color images of size 32√ó32 pixels, each belonging\n",
    "to one of ten categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship,\n",
    "and truck.\n",
    "\n",
    "State-of-the-art deep learning methods for this dataset are as good as humans at classifying these images. In this section we start off with much simpler methods that will\n",
    "run relatively quickly. Then, we discuss briefly what the gap is between these and the\n",
    "state of the art.\n",
    "\n",
    "Proposed neural network model consists of three blocks of convolutional layers, followed by the fully connected and output layers we have already seen a few times before. Each block of convolutional layers contains three consecutive convolutional layers, followed by a single pooling and dropout. This model is still compact\n",
    "and fast, and achieves approximately 83% accuracy after ~15 epochs. However, there is some overfitting as this accuracy is not found during testing phase.\n",
    "\n",
    "**$7^{th}$ Example CIFAR10**\n",
    "\n",
    "Please restart the Jupyter notebook to clean the used memory and run only this example after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from callbackPlots import AccuracyPlotCallback\n",
    "from callbackPlots import count_neurons_and_synapses\n",
    "        \n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize the data to [0, 1] range\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Create the ImageDataGenerator instance\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Fit the generator on the training data\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Define the model using the Sequential API\n",
    "model = models.Sequential([\n",
    "    # First Convolutional Layer\n",
    "    layers.Conv2D(30, (3,3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Second Convolutional Layer\n",
    "    layers.Conv2D(50, (3,3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Third Convolutional Layer\n",
    "    layers.Conv2D(80, (3,3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(pool_size=(8, 8)),\n",
    "\n",
    "    # Flatten the output from the convolutional layers\n",
    "    layers.Flatten(),\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    layers.Dense(500, activation='relu'),\n",
    "\n",
    "    # Dropout layer to reduce overfitting\n",
    "    layers.Dropout(0.8),\n",
    "\n",
    "    # Output layer with softmax activation\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define the Adam optimizer with an explicit learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1.0e-6)  # Adjust this value as needed\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model summary to check architecture\n",
    "model.summary()\n",
    "\n",
    "# Create the callback instance\n",
    "plot_callback = AccuracyPlotCallback()\n",
    "\n",
    "# Debugging: Print if the callback is created correctly\n",
    "print(f\"Callback created: {plot_callback}\")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=20, batch_size=64, validation_data=(x_test, y_test), \n",
    "                    callbacks=[plot_callback, early_stopping, reduce_lr])\n",
    "#history = model.fit(x_train, y_train, epochs=20, batch_size=64, validation_data=(x_test, y_test), callbacks=[plot_callback])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"\\nTest accuracy: {test_acc:.4f}\")\n",
    "# Save the model in Keras format (.keras)\n",
    "model_filename = f\"model_Accu{test_acc:.4f}.keras\"\n",
    "model.save(model_filename)\n",
    "\n",
    "# Call the function\n",
    "count_neurons_and_synapses(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "It is powerful class of deep learning algorithms particularly useful and popular in natural language processing. Besides, RNN is an immensely important and useful type of neural network to process sequential structures. The type of data with strong sequential structure is natural language. That's why RNN is often employed in a supervised text classification\n",
    "problem with word-embedding training.\n",
    "\n",
    "The basic idea behind RNN models is that each new element in the sequence contributes some new information, which updates\n",
    "the current state of the model. A fundamental mathematical construct in statistics and probability, which is often\n",
    "used as a building block for modeling sequential patterns via machine learning is the\n",
    "Markov chain model. RNN applies some form of ‚Äúloop‚Äù to deal with such sequency of information. This is very related to the Markov chain models discussed previously and their\n",
    "hidden Markov model (HMM) extensions, which are not discussed here.\n",
    "\n",
    "The update step for our simple vanilla RNN is\n",
    "\n",
    "$h_t = tanh(W_x x_t + W_h h_{t-1} + b)$\n",
    "\n",
    "where $W_h$, $W_x$, and $b$ are weight and bias variables we learn, $tanh(¬∑)$ is the hyperbolic\n",
    "tangent function.\n",
    "\n",
    "While the structure of natural images is\n",
    "well suited for CNN models, it is revealing to look at the structure of images from\n",
    "different angles. In a trend in cutting-edge deep learning research, advanced models\n",
    "attempt to exploit various kinds of sequential structures in images, trying to capture\n",
    "in some sense the ‚Äúgenerative process‚Äù that created each image. Intuitively, this all\n",
    "comes down to the notion that nearby areas in images are somehow related, and trying to model this structure. We assume that the last state vector has ‚Äúaccumulated‚Äù information representing the entire sequence.\n",
    "\n",
    "Following example builds a vanilla RNN example. The model we defined is composed of two main layers:\n",
    "1. **VanillaRNNLayer**  \n",
    "   - A custom layer that processes the input sequence over 28 time steps.\n",
    "   - For each time step, it updates a hidden state of size 128 using the recurrence:\n",
    "    \n",
    "    $h_t = tanh(W_x x_t + W_h h_{t-1} + b)$\n",
    "\n",
    "    where:\n",
    "    - $W_h$ (hidden-to-hidden weights) has a shape of \\([128, 128]\\).\n",
    "    - $W_x$ (input-to-hidden weights) maps the input vector (of size 28) to a hidden state vector of size **128**.\n",
    "    - $b$ is the bias term.\n",
    "\n",
    "   - This layer encapsulates the recurrent processing logic using the parameters:\n",
    "     - `element_size` (input dimension per time step)\n",
    "     - `time_steps` (length of the sequence)\n",
    "     - `hidden_layer_size` (size of the hidden state)\n",
    "\n",
    "2. **Dense Layer**  \n",
    "   - A fully connected layer that takes the final hidden state (size 128) as input.\n",
    "   - It produces **num_classes = 10** logits for classification.\n",
    "\n",
    "\n",
    "\n",
    "The **VanillaRNNLayer** is designed with a hidden state of size **128** (`hidden_layer_size = 128`). This means that at every time step, the hidden state vector \\( h_t \\) has **128 values**. Each value represents a neuron in the recurrent layer. Because the hidden state $h_t$ is of size **128**, this confirms that the recurrent layer effectively contains **128 neurons**.\n",
    "\n",
    "Thus, the recurrent layer in your model indeed has **128 neurons**. This layer is followed by a fully connected layer of **10 neurons**, which produces the **10** output logits required for a MNIST classification.\n",
    "\n",
    "**$8^{th}$ Example MNIST classification using a vanilla RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from callbackPlots import AccuracyPlotCallback\n",
    "\n",
    "# Parameters\n",
    "element_size = 28\n",
    "time_steps = 28\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "hidden_layer_size = 128\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize and reshape dataset for RNNs\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = x_train.reshape(-1, time_steps, element_size)  \n",
    "x_test = x_test.reshape(-1, time_steps, element_size)\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Create dataset batches\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "\n",
    "# Define a custom vanilla RNN model using subclassing\n",
    "class VanillaRNNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_layer_size, **kwargs):\n",
    "        super(VanillaRNNLayer, self).__init__(**kwargs)\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        element_size = input_shape[-1]  # Get element size dynamically\n",
    "\n",
    "        # Define Weights and Biases\n",
    "        self.Wx = self.add_weight(name=\"Wx\", shape=(element_size, self.hidden_layer_size),\n",
    "                                  initializer='random_normal', trainable=True)\n",
    "        self.Wh = self.add_weight(name=\"Wh\", shape=(self.hidden_layer_size, self.hidden_layer_size),\n",
    "                                  initializer='random_normal', trainable=True)\n",
    "        self.b_rnn = self.add_weight(name=\"b_rnn\", shape=(self.hidden_layer_size,),\n",
    "                                     initializer='zeros', trainable=True)\n",
    "        super(VanillaRNNLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        time_steps = tf.shape(inputs)[1]  # Dynamically get time steps\n",
    "\n",
    "        hidden_state = tf.zeros([batch_size, self.hidden_layer_size])\n",
    "\n",
    "        for t in range(time_steps):\n",
    "            x_t = inputs[:, t, :]\n",
    "            hidden_state = tf.tanh(\n",
    "                tf.matmul(hidden_state, self.Wh) +\n",
    "                tf.matmul(x_t, self.Wx) +\n",
    "                self.b_rnn\n",
    "            )\n",
    "\n",
    "        return hidden_state\n",
    "\n",
    "class VanillaRNN(tf.keras.Model):\n",
    "    def __init__(self, hidden_layer_size, num_classes, **kwargs):\n",
    "        super(VanillaRNN, self).__init__(**kwargs)\n",
    "        self.rnn_layer = VanillaRNNLayer(hidden_layer_size)\n",
    "        self.dense = tf.keras.layers.Dense(num_classes)  # No activation to keep logits\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.rnn_layer(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits  # Return unnormalized scores (logits)\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "\n",
    "vanilla_rnn_model = VanillaRNN(hidden_layer_size=hidden_layer_size, num_classes=num_classes)\n",
    "\n",
    "# Optionally, build the model to see the summary:\n",
    "vanilla_rnn_model.build(input_shape=(None, time_steps, element_size))\n",
    "\n",
    "# Create the callback instance\n",
    "plot_callback = AccuracyPlotCallback()\n",
    "\n",
    "# Debugging: Print if the callback is created correctly\n",
    "print(f\"Callback created: {plot_callback}\")\n",
    "\n",
    "# Compile & Train\n",
    "vanilla_rnn_model.compile(optimizer='adam',\n",
    "                          loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "vanilla_rnn_model.fit(train_dataset, epochs=10, validation_data=test_dataset, callbacks=[plot_callback])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = vanilla_rnn_model.evaluate(x_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "vanilla_rnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$9^{th}$ Example MNIST classification Simple RNN from Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from callbackPlots import AccuracyPlotCallback\n",
    "from callbackPlots import count_neurons_and_synapses\n",
    "\n",
    "# Define some parameters\n",
    "element_size = 28\n",
    "time_steps = 28\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "hidden_layer_size = 128\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize images to [0,1] range\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Reshape the dataset for RNNs\n",
    "x_train = x_train.reshape(-1, time_steps, element_size)  # (batch, 28, 28)\n",
    "x_test = x_test.reshape(-1, time_steps, element_size)\n",
    "\n",
    "\n",
    "# Convert labels to categorical (if needed)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Create TensorFlow dataset for batching\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential([\n",
    "    layers.SimpleRNN(hidden_layer_size, activation=\"tanh\", input_shape=(time_steps, element_size)),\n",
    "    layers.Dense(num_classes, activation=\"softmax\")  # Fully connected output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],)  # Explicitly add metrics\n",
    "\n",
    "# Create the callback instance\n",
    "plot_callback = AccuracyPlotCallback()\n",
    "\n",
    "# Debugging: Print if the callback is created correctly\n",
    "print(f\"Callback created: {plot_callback}\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=10, validation_data=test_dataset,callbacks=[plot_callback])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"\\nTest accuracy: {test_acc:.4f}\")\n",
    "# Save the model in Keras format (.keras)\n",
    "model_filename = f\"model_Accu{test_acc:.4f}.keras\"\n",
    "model.save(model_filename)\n",
    "\n",
    "# Model summary\n",
    "model.summary() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-npu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
